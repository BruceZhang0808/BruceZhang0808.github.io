---
date : '2025-12-06'
draft : false
title : '关于NEO的笔记'
subtitle : ""
description : ""
author : 'BruceZhang'
categories : ["Remote Sensing"]
tags : ["NEO"]
summary: "NEO能否用于遥感领域？"
---

## 1. 这篇论文总体在干什么？

论文的题目是 **“From Pixels to Words – Towards Native Vision-Language Primitives at Scale”**，核心目标是：

> 能不能做出一种**真正原生的一体化多模态模型**（Native VLM），
> 不用「视觉编码器 + 投影层 + LLM」的模块化堆叠方式，
> 但在性能上**不输甚至逼近**这些主流模块化 VLM，同时**训练成本更低**？

为此他们提出了一个叫 **NEO** 的家族模型，并抽象出一个“**原生视觉–语言基元（native VLM primitive）**”作为通用积木，把「编码图像」「对齐图像和文本」「做多模态推理」统一塞进同一个模块里。

---

## 2. 他们觉得传统方法有什么问题？

### 2.1 模块化 VLM 的问题（CLIP+LLM 那一路）

典型结构：
**Vision Encoder（VE）+ Projector + LLM**，比如 InternVL、Qwen-VL、GPT-4o 这一类。

存在的问题：

1. **训练流程很复杂**

   * 要先训视觉编码器，再对齐图像和文本，再做指令微调，通常是多阶段流水线。

2. **视觉部分有很强的先验偏置**

   * 预训练 ViT 通常固定分辨率、固定 patch size、tile 方式，很难灵活适配任意分辨率与长宽比，细节有损失。

3. **视觉与语言参数如何平衡很麻烦**

   * VE 和 LLM 各自有容量，怎么分算合理？改其中一个经常影响整体效果。

简单说：**表现强，但架构重、训练贵、维护难。**

### 2.2 既有 “原生” VLM 的问题

Fuyu、EVE、Chameleon、SOLO 等尝试把图像直接喂进一个 Transformer，做**早期融合（early fusion）**的一体化模型。

问题在于：

* 直接把图像 patch 当「词」来处理，会：

  * **破坏原本 LLM 的语言能力**；
  * 或者对图像的学习效率很差，需要**超大数据量**才能拉上来；
  * 有的用 MoE / DaC 把视觉和语言拆专家模块，但**视觉和语言各自的优势很难统一到一个干净的结构里**。

所以作者提出的问题是：

> 怎样设计一个「原生多模态积木」，
> 既兼容 LLM 的习惯，又有 ViT 的空间建模能力，
> 还要易训练、易扩展？

---

## 3. 他们具体提出了什么新东西？

核心关键词：**Native VLM Primitive + Pre-Buffer / Post-LLM + Native-RoPE + Mixed Attention**。

我按模块拆开讲。

---

### 3.1 Patch & Word Embedding：把图像和文字都变成 token

**专有名词：Patch Embedding Layer (PEL)**
→ 用两个卷积 + GELU 把图像切成 patch，并映射到特征空间。

* Conv1 stride = 16, Conv2 stride = 2，所以**每个视觉 token 对应一个 32×32 像素块**。
* 再加上 **2D 正弦位置编码（2D Sinusoidal PE）**

  * 通俗说：给每个 patch 加上一个「在图像中是第几行第几列」的坐标标签。
* 在视觉 token 序列的前后插入特殊 token：`<img>` 和 `</img>`

  * 类似于把一段图像区域当作「句子」，用开始/结束符包起来。

**文字部分**使用原始 LLM 的 tokenizer → Word Embedding Layer (WEL)。

之后：

> 图像 token + 文本 token 串在一起，
> 一起喂进统一的 Transformer backbone。

---

### 3.2 Native VLM Primitive：统一的“多模态积木块”

**专有名词：Native VLM Primitive**
→ 本质上是一个改造过的 Transformer block，包含：

* **RMSNorm**（一种归一化层，像 LayerNorm 的变体）；
* **SwiGLU 前馈网络（Feed-Forward Network, FFN）**；
* **Multi-Head Native Attention (MHNA)**：
  多头注意力，但对时间维 T、空间 H/W 分头建模。

关键设计：

1. **扩展 Q/K 的头维度，拆成 T / H / W 三种关系**

   * T = 文本序列位置或视频时间帧
   * H/W = 图像的行/列位置
   * 这样可以**同时建模：文字顺序、图像局部结构、图像–文字之间的关联**。

2. **H/W 对应的 Key 权重一开始全 0 初始化**

   * 通俗说：一开始只保留 LLM 原本的“语言模式”；
   * 视觉相关的那部分逐渐在训练中“长出来”，避免一上来就把 LLM 搞乱。

---

### 3.3 Native-RoPE：给 T/H/W 三个维度分别配“坐标系”

**专有名词：RoPE（Rotary Position Embedding）**
→ 一种常用的位置编码方式，通过在 Q/K 上旋转向量，把「相对位置」编码到注意力中。

传统 1D-RoPE 只有一个序列维（T），但对于图像/视频，多了 H/W/T 三个维度。
已有一些 3D-RoPE 工作，但作者认为都存在**频率和维度混在一起**的问题。

他们提出：

> **Native-RoPE：为 T、H、W 分别单独分配通道和基频**，
> 让时间和空间的“位置感”互不干扰，又兼容原本 LLM 的设定。

关键点：

1. **索引分配（Index Allocation）**

   * 文本：只有 T 有索引，H/W 置 0 → 对 LLM 来说，和原来 1D-RoPE 一样。
   * 图像：T 对整张图固定，H/W 根据行列变化 → 强调空间结构。
   * 多模态时，各模态的 T 会按顺序接上，保证序列整体有清晰的时间/顺序关系。

2. **频率与通道分配（Channel & Frequency Allocation）**

   * T 用原本 LLM 的频率（高/低频兼顾 → 既看近邻词又看长程依赖）。
   * H/W 用新的更高频率（更适合建模图像局部细节）。
   * 这样可以：

     * 不破坏 LLM 原有语言模式；
     * 又给视觉专门分一块频段，专门用来刻画图像的空间关系。

直观上：

> 把「时间轴」和「图像平面」的坐标系拆开，
> 各用一套频率参数来编码，互不拖累。

---

### 3.4 Mixed Attention：图像双向、自回归文本

**专有名词：Native Multi-Modal Attention with mixed masking**

注意力的 mask 策略是：

* **文本 token**：标准的**因果注意力（causal attention）**

  * 只能看前面的 token，保证语言生成是自回归的。
* **图像 token**：**全双向注意力（bidirectional attention）**

  * 任意 patch 都可以看所有 patch，就像 ViT 的自注意力。

这样可以：

* 图像内部：学习丰富的空间结构与上下文；
* 图像-文本之间：通过同一层注意力融合；
* 文本生成：维持正常的语言自回归逻辑。

为了效率，他们使用 **FlexAttention** 做块状注意力加速，减少显存和时间开销。

---

### 3.5 Pre-Buffer & Post-LLM：训练阶段的“分体 + 合体”

**专有名词：Pre-Buffer / Post-LLM**

* **Pre-Buffer**：前半部分层，专门负责把图像+文本映射到统一的多模态表示空间，更偏“视觉+对齐”。
* **Post-LLM**：后半部分层，更偏「语言理解 + 推理」，尽量沿用原 LLM 的能力。

训练策略：

1. **预训练阶段（Pre-training）**

   * 冻结 LLM 的参数，只训练：

     * patch embedding；
     * pre-Buffer；
     * 新增的 Q/K 头和 H/W 相关参数。
   * 目标：

     > 在 LLM 保持稳定的前提下，
     > 让视觉部分学会基本感知能力，并和语言建立初步对齐。

2. **中期训练（Mid-training）**

   * 整个模型开始**端到端训练**，
   * 用多种 caption / QA / detection / OCR 数据补全视觉语言对齐。

3. **监督微调（SFT）阶段**

   * 进一步在高质量指令数据上做多任务微调（对话、VQA、数学、知识问答等）。

训练结束后：

> Pre-Buffer 和 Post-LLM 在结构上**合并成一个单体 backbone**，
> 模型会自动在不同层分配「感知」「对齐」「推理」的能力。

---

## 4. 他们解决了哪些问题？

可以概括为四点：

1. **架构统一但不牺牲性能**

   * NEO 在 2.2B 和 9B 两个规模上，用 **比主流模块化 VLM 少得多的数据量**（345M + 40M + 4M），在 MMMU、MMBench、VQA 等基准上已经接近甚至逼近同级别模块化模型。

2. **像素–词汇对齐更自然**

   * 得益于 Native-RoPE 和 Mixed Attention，图像 patch 与文本 token 在同一个注意力空间里交互，
   * 对**图表、文档、OCR-heavy 任务**表现很强。

3. **训练流程更简单、可重用性更高**

   * 预训练阶段只调 pre-Buffer 和 H/W 相关部分，LLM 冻结：

     * 保证语言能力稳定；
     * 训练成本比全量从头训更低。
   * 预训练好的 pre-Buffer 可以作为社区可复用模块，后续想接别的 LLM 也比较容易。

4. **为原生 VLM 设计了一套“第一性原理”的标准**
   他们总结出一个 native VLM primitive 应该满足的三条原则：

   1. 在一个统一空间里对齐像素和词；
   2. 在结构上自然融合视觉模块和语言模块各自的优势；
   3. 内在就支持多模态编码、对齐和推理，而不是外面拼一堆模块。

---

## 5. 与传统方法的差异总结

### 5.1 对比模块化 VLM（CLIP+LLM 体系）

* **传统**：

  * 有独立 **Vision Encoder**（ViT/ConvNet）、独立 **LLM**，
  * 中间靠 Projector 或 Cross-Attention 接起来，
  * 经常需要 tile + 分辨率处理。

* **NEO**：

  * 把图像 patch 和文本 token 一起交给一个统一的 Transformer 处理；
  * 通过专门的 Native-RoPE 和 Mixed Attention，让这个 Transformer 同时具备：

    * 像 ViT 一样处理图像；
    * 像 LLM 一样处理文本。

直观对比：

> 以前是「两个大模型 + 一个转接头」，
> 现在是「一个大模型，内部自然支持多模态」。

### 5.2 对比之前的原生 VLM（Fuyu / EVE / Chameleon / SOLO 等）

* 以前的做法：

  * 图像通常简单变成 1D 序列，RoPE 也只有 1D；
  * 视觉和语言的 RoPE 参数、注意力设计**没有针对 T/H/W 的差异做精细设计**；
  * 常常要依赖很大的数据，才能勉强追上模块化模型。

* NEO 引入：

  * **三维解耦 RoPE**：T、H、W 三套频率 + 通道；
  * **新的 Q/K head 扩展**，且用零初始化控制干扰；
  * **预训练阶段的 Pre-Buffer 方案**，让视觉在不伤害语言能力的前提下逐步“长出来”。

实验上，NEO 在各种 VQA 和通用 VL 基准上，**显著超过当前原生 VLM 家族**，而且使用的数据规模更小。

---

## 6. 用了哪些数据集和评测基准？

### 6.1 预训练数据（345M image–text pairs）

* **LAION-400M**：

  * 100M 英文 + 20M 中文 image–caption 对；
* **COYO-700M**：

  * 150M 英文 pair；
* **BLIP3-O 生成的长描述数据**：

  * 20M 长 caption；
* **OpenImages**：

  * 5M 图像，用 InternVL2-8B 重新生成短 caption；
* **LAION-COCO**：

  * 30M 样本；
* **Wukong（悟空数据集）**：

  * 20M 样本，含丰富 OCR 标注。

这一步只训练 PEL + pre-Buffer + 新增的 Q/K 头，LLM 冻结。

### 6.2 中期训练（40M 样本）

来自 **InternVL-1.5** 的预训练语料，包括：

* 图像 caption：约 66%；
* 多模态对话：约 11%；
* 检测：约 8%；
* OCR：约 15%。

目的是提升高分辨率、复杂场景、多对象和 OCR 的能力。

### 6.3 监督微调（4M 指令）

* 大约 **4M 双语高质量指令数据**，
* 覆盖任务：VQA、多轮对话、数学题、知识推理等。

### 6.4 评测基准

他们用 **VLMEvalKit** 在一系列公开基准上测：

* **通用多模态推理**

  * MMMU、MMBench-EN、MMVet、MMStar、SEEDBench-IMG；
* **幻觉诊断**

  * POPE、HallusionBench；
* **图表 / 文档理解、OCR**

  * AI2D、DocVQA、ChartQA、InfoVQA、TextVQA、OCRBench。

结果表明，在 2B 和 8B 两个规模上，NEO：

* **接近或逼近** 同级别模块化 VLM（Qwen2-VL、InternVL2.5、Qwen2.5-VL、InternVL3）；
* **显著优于** 同级别原生 VLM（Fuyu、EVE、Chameleon、SOLO、BREEN、SAIL 等）。

---

## 7. 如果用一句话总结这篇论文

> 他们从「第一性原理」出发，
> 设计了一种统一的、三维位置编码 + 混合注意力的多模态 Transformer 积木，
> 再通过分阶段训练策略，让这个“原生 VLM”在有限数据下
> 既学会看图，又保住了语言能力，
> 并在一堆标准 benchmark 上证明：
> **原生多模态架构完全有可能在性能和效率上，匹敌主流模块化方案。**

我直接把这篇 NEO 论文的**创新点**先浓缩出来，再一条条说明对遥感（尤其是多模态 / 大模型方向）有什么**具体启示**。

---

## 一、这篇论文的关键创新点（从遥感视角重述）

### 1. 原生多模态积木：Native VLM Primitive

**创新点：**
提出了一种统一的 Transformer 积木（native VLM primitive），在**同一个 block**里同时完成：

* 图像编码（视觉特征提取）
* 文本编码（语言特征）
* 图文对齐（pixel–word 对齐）
* 多模态推理（问答、推理等）

这个积木的核心是：

* **Multi-Head Native Attention（MHNA）**：专门把注意力的“头”拆成负责时间 T、空间 H/W 的不同子空间；
* **SwiGLU FFN** 等现代 LLM 组件，保证语言能力不被破坏。

> 直白说：不再是 “一个 ViT + 一个 LLM + 一个投影层” 的三段式，而是“一颗大脑里长出了视觉和语言两个半球，天生连在一起”。

---

### 2. 三维位置编码：Native-RoPE（T/H/W 解耦）

**创新点：**
提出 **Native-RoPE**——把 RoPE（旋转位置编码）从 1 维扩展到“**时间 T + 空间 H + 空间 W**”，并且给这三维分别分配频率和通道。

* 文本：只在 T 维有位置（等价于原 LLM 的 RoPE），保证语言能力原样保留；
* 图像：T 固定，H/W 变化 → 强调 2D 空间结构；
* 不同维度用不同频率，避免“时间感”和“空间感”混在一起互相干扰。

> 这给了一个很干净的范式：**如何在一个模型里同时建模“序列时间”和“二维空间”**。

---

### 3. 混合注意力与掩码：图像双向、文本自回归

**创新点：**
在同一个注意力层中应用**不同的 mask 规则**：

* 文本 token：使用因果掩码（只能看前面），保持自回归生成；
* 图像 token：使用全双向注意力（任意 patch 互看），像 ViT 那样建模全局空间关系。

> 结果是：**图像内部可以自由交流，文本仍然按语言规律一个字一个字往后写，两者在同一层交互**，不需要额外 cross-attention “桥接”。

---

### 4. 分阶段的 Pre-Buffer / Post-LLM 训练范式

**创新点：**
把整个网络在训练时临时拆成：

* **Pre-Buffer**：前半部分层，更偏视觉和图文对齐；
* **Post-LLM**：后半部分层，保留原始语言能力。

训练流程：

1. 先**冻结 LLM**，只训练 PEL + Pre-Buffer + H/W 新参数 → 让视觉能力长出来；
2. 再逐步解冻，端到端训练整模；
3. 最终再合并成一个统一 backbone。

> 好处：不一上来就把 LLM 搞坏，训练更稳定、成本更低，而且 Pre-Buffer 本身可以当“通用视觉前端”复用。

---

### 5. 数据效率：相对小的数据量接近模块化 VLM 的性能

**创新点：**

* 用 **3.45 亿 图文对预训练 + 4000 万多模态中期训练 + 400 万指令微调**，在 2.2B 和 9B 参数规模上，在 MMMU、MMBench、VQA、OCR 等基准上**逼近甚至接近同级别模块化 VLM**；
* 同时**大幅超越现有原生 VLM**（Fuyu、EVE、Chameleon、SOLO 等），说明原生架构只要设计得当，并不一定“注定弱于”模块化路线。

> 对任何想做「领域原生多模态大模型」的人，这是一个很强的“可行性证明”。

---

## 二、这些创新对遥感领域有哪些具体启示？

下面我就按“遥感能做什么”来对照这些创新，说成你可以直接往论文 / 课题里写的那种思路。

---

### 启示 1：给“遥感原生 VLM / RS-Foundation Model”提供了清晰架构模板

现在遥感基础模型多是：

* ViT / Swin backbone（只会看图）
* * 各种 decoder 做分类 / 分割 / 检测
* 想做多模态（图 + 文）时再拼一个 CLIP / LLM。

NEO 提供的模板是：

> 用 **native VLM primitive** 造一个「**统一 RS 多模态 backbone**」：
> 一次性支持 ——
>
> * 纯视觉任务：语义分割、目标检测、变化检测；
> * 图文任务：检索、VQA、Caption、报告生成。

遥感更适合的变体是：

* 把图片 patch + SAR patch + 多光谱 bands 都当成不同“模态 token”喂进 backbone；
* 文本部分可以是：地块描述、POI 标签、规划条文、灾情简报。

NEO 证明了这种一体化架构是**可训练、可扩展、性能不差的**，这为“RS-native VLM”提供了非常强的理论支撑。

---

### 启示 2：Native-RoPE 可以直接演化为“时–空–谱–模态”统一编码

NEO 的 Native-RoPE 将 T/H/W 解耦，是对“**时空多维编码**”的一次很干净的处理方式。

在遥感里，你可以直接做：

* T：多时相（年际变化、灾前灾后、多季节作物周期）；
* H/W：空间位置（大范围拼接场景）；
* 额外一维：谱段 / 传感器（RGB、多光谱、SAR、LiDAR、高程 DEM 等）。

这样你可以在**同一套 RoPE 框架下**：

* 端到端建模“某一像素在不同时间、不同传感器、不同高度维度”的联合演化；
* 无缝支持：

  * 多时相语义分割、作物分类；
  * 光学+SAR 联合分割 / 检测；
  * 多源融合变化检测。

> 换句话说，Native-RoPE 给了你一个可以直接改写成“**遥感版 4D 位置编码**”的数学模板。

---

### 启示 3：像素–词语统一空间 → 遥感 open-vocabulary / prompt-based 语义分割

NEO 最强的概念之一是：**所有视觉 patch 和词语都在同一个 embedding 空间里对齐学习**。

直接迁移到遥感，就是：

* 把“建筑 / 道路 / 工业园 / 港口 / 光伏电站”等类别当作**文本标签**（label-as-text）；
* 训练时同时用：

  * 像素级 CE / Dice loss；
  * 像素–词语对比 loss（让建筑像素嵌入靠近“building”“residential block”等词）。

然后你可以做：

1. **开集 / 少样本语义分割**

   * 新类别只需要写几个 prompt（“logistics hub with large warehouses”），少量标注，就能在原空间里插入新类；
2. **跨数据集统一标签体系**

   * 不同数据集如果类别定义不同，通过文本描述可统一到相近语义；
3. **分割 + 文本解释一体化**

   * 模型不仅给类别 ID，还能解释“该区域为高层住宅区，密度较高，周围有多条主干道”。

> 对比传统“纯 one-hot 标签”的分割，NEO 式 pixel–word 对齐，天生支持 open-vocabulary 和多任务融合。

---

### 启示 4：Mixed Attention 的 mask 设计可用于“图像双向、时序/文本自回归”的 RS 任务

遥感经常有两类需求：

* 图像内部：需要强空间上下文（扩展范围、连片农业、城市结构）；
* 文本/时间：往往有顺序逻辑（多时相序列、时序描述、问答）。

NEO 在一个注意力层里混合使用双向图像注意力 + 自回归文本注意力，说明：

> 可以不必为“看图”和“讲话”分两套注意力塔，
> 而是在一个层里，用 mask 区分 **“谁可以看谁”**。

这对遥感的启示包括：

* 在**多时相序列**上，可以设定：

  * 图像 patch 之间是双向（方便抓全局）；
  * 时间顺序 / 文本描述部分是有因果的（避免“偷看未来帧”）。
* 对于**时序解读任务**（比如“描述 2010–2025 年城市扩张过程”），可以在一个 unified attention 里同时建模“图像发展” + “文字叙述”。

---

### 启示 5：Pre-Buffer 训练范式非常适合做“遥感通用前端”

遥感有一个天然条件：**海量无标注卫星影像**，但高质量标注和图文对少。

NEO 的 Pre-Buffer 做法本身就是：

* 先冻结 LLM，只训练前半截，让模型学会视觉感知与基础对齐；
* 这部分在训练完之后可以当**通用视觉–语言前端**复用。

这为“遥感原生多模态前端”提供一条现实路径：

1. 用大规模遥感影像（光学+SAR+多谱段等）做自监督 / 弱监督，训练一个 RS-PreBuffer；
2. 上面挂：

   * 分割头（语义分割）；
   * 检测/目标提取头；
   * 变化检测头；
   * 文本交互头（VQA、Caption、检索）。

这种拆分方式适配遥感现状：

> “图像数据极多、文本/标注有限”，
> 可以先把 RS-PreBuffer 当成一个纯视觉+少量文本的“预处理大脑”，再逐步接 LLM 或任务头。

---

### 启示 6：原生 VLM 的成功，是“做遥感领域原生多模态模型”的强背书

最后，从方法论层面看，这篇论文本身给了遥感界两个信号：

1. **原生多模态架构并不是玩具，而是可与主流模块化方案竞争的**

   * NEO 在一系列基准上接近或逼近 Qwen2-VL、InternVL3 等模块化模型；
   * 这说明如果做“RS-Native VLM”，不必总是从 CLIP + LLM 拼起，可以尝试一体化框架。

2. **精心设计架构 + 适当规模数据，就能省钱又好用**

   * 它没有用超级夸张的 10B 级图文数据，而是通过 Native-RoPE + MHNA + Pre-Buffer 把数据利用率提高了。
   * 遥感界完全可以用“**几亿级无标注影像 + 少量文本/标注**”，做一个中等规模但专门适配 RS 的原生多模态大模型。

---

如果你接下来有兴趣，我可以基于这些启示，帮你构造几个**可以直接写进开题/论文里的题目和技术路线**，比如：

* “RS-NativeSeg: 基于 Native VLM Primitive 的遥感语义分割原生骨干”
* “Label-as-Text 遥感开集语义分割：从像素到词语的统一空间”
* “Pre-Buffer RS-VLM: 基于多源遥感影像的原生多模态预训练框架”

这样你会有比较清晰的“从 NEO 到自己课题”的路径。
